# Visual Multi-Agent QA System

Há»‡ thá»‘ng Ä‘a tÃ¡c nhÃ¢n (multi-agent) cho tráº£ lá»i cÃ¢u há»i vá» hÃ¬nh áº£nh, káº¿t há»£p computer vision vá»›i kháº£ nÄƒng tÃ¬m kiáº¿m tri thá»©c.

## ğŸš€ TÃ­nh nÄƒng

- **Multi-Agent Architecture**: Junior, Senior, vÃ  Manager analysts
- **Visual Question Answering**: Sá»­ dá»¥ng Describe Anything Model (DAM)
- **Knowledge Retrieval**: TÃ­ch há»£p ArXiv, Wikipedia, DuckDuckGo
- **LangGraph Orchestration**: Workflow management vÃ  state tracking
- **Session Memory**: LÆ°u trá»¯ tráº¡ng thÃ¡i conversation

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
src/
â”œâ”€â”€ agents/          # Agent definitions
â”œâ”€â”€ core/           # Core workflow & orchestration  
â”œâ”€â”€ models/         # Data models & state definitions
â”œâ”€â”€ tools/          # VQA & knowledge tools
â”œâ”€â”€ utils/          # Utility functions
â”œâ”€â”€ evaluation/     # Evaluation & testing
â””â”€â”€ main.py        # Entry point
```

CUDA_VISIBLE_DEVICES=2 vllm serve Qwen/Qwen3-1.7B \
--port 1234 \
--dtype auto \
--gpu-memory-utilization 0.6 \
--max-model-len 4096 \
--enable-auto-tool-choice \
--tool-call-parser hermes \
--trust-remote-code

vllm serve Qwen/Qwen3-1.7B \
  --port 1234 \
  --dtype auto \
  --gpu-memory-utilization 0.5 \
  --max-model-len 65536 \
  --rope-scaling '{"rope_type":"yarn","factor":2.0,"original_max_position_embeddings":32768}' \
  --enable-auto-tool-choice \
  --tool-call-parser hermes \
  --trust-remote-code


## ğŸ› ï¸ CÃ i Ä‘áº·t

```bash
# Clone repository
git clone <repo-url>
cd visual-multi-agent-qa

# Install dependencies  
pip install -r requirements.txt

# Set up environment
cp .env.example .env
# Edit .env with your API keys
```

## ğŸ¯ Sá»­ dá»¥ng

### 1. Command Line

```bash
python src/main.py
```

### 2. Programmatic

```python
from src.main import run_visual_qa

result = run_visual_qa(
    question="What color is the dog's fur?",
    image_url="https://example.com/dog.jpg",
    thread_id="session_1"
)
print(result)
```

### 3. Jupyter Notebook

```python
import os
os.environ['JUPYTER_RUNNING'] = '1'

from src import run_visual_qa

# Analyze image
result = run_visual_qa(
    "What breed is this dog?", 
    "path/to/image.jpg"
)
```

## ğŸ”§ Cáº¥u hÃ¬nh

### API Configuration

```python
# .env file
OPENAI_API_KEY=your_openai_key
OPENAI_BASE_URL=http://127.0.0.1:1234/v1  # Local LLM
```

### Agent Configuration

Chá»‰nh sá»­a `src/main.py` Ä‘á»ƒ tÃ¹y chá»‰nh analysts:

```python
def create_analysts():
    # Customize agent roles, tools, prompts
    pass
```

## ğŸ“Š VÃ­ dá»¥

```python
# Basic visual QA
result = run_visual_qa(
    "What's in this image?",
    "https://example.com/image.jpg"
)

# Multi-perspective analysis  
result = run_visual_qa(
    "Analyze the technical details of this architecture",
    "building.jpg",
    thread_id="architecture_analysis"
)
```

## ğŸ§ª Testing

```bash
# Run tests
python -m pytest src/evaluation/

# Run specific test
python src/evaluation/test_agents.py
```

## ğŸ“ˆ Performance

- **Junior Agent**: Basic VQA (~1-2s)
- **Senior Agent**: VQA + Knowledge (~3-5s)  
- **Manager Agent**: Comprehensive analysis (~5-10s)

## ğŸ” Debugging

Báº­t debug mode:

```python
import logging
logging.basicConfig(level=logging.DEBUG)

result = run_visual_qa(question, image, thread_id="debug")
```

## ğŸ“ License

MIT License

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch
3. Submit pull request

## ğŸ“ Support

- Issues: [GitHub Issues](link)
- Documentation: [Wiki](link)
- Email: support@example.com 